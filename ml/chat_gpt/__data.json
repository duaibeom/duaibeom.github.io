{"type":"data","nodes":[null,{"type":"data","data":[{"prev":1,"next":4,"section":7},{"slug":2,"title":3},"ml\u002Fvision_transformer","Vision Transformer",{"slug":5,"title":6},"ml\u002Fpytorch","PyTorch",{"file":8,"title":9,"date":10,"summary":-1,"tag":11,"toc":14,"body":15},"ml\u002F20-chat_gpt.md","Learn ChatGPT from ChatGPT","Feb. 3, 2023",[12,13],"GPT","OpenAI","\u003Ch4\u003EContents\u003C\u002Fh4\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#understand-chatgpt-technology\"\u003EUnderstand ChatGPT Technology\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#attention-is-all-you-need\"\u003EAttention is All you Need\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#gpt\"\u003EGPT\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#gpt-vs-transformer\"\u003EGPT vs. Transformer\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#text-and-code-embeddings-by-contrastive-pre-training\"\u003EText and Code Embeddings by Contrastive Pre-Training\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#autoregressive-model\"\u003EAutoregressive model\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#zero-shot-learning\"\u003EZero-Shot learning\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#min-gpt\"\u003Emin GPT\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#pros-and-cons\"\u003EPros and Cons\u003C\u002Fa\u003E\u003C\u002Fp\u003E","\u003Ch3 id=\"understand-chatgpt-technology\"\u003EUnderstand ChatGPT Technology\u003Ca href=\"#understand-chatgpt-technology\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003EChatGPT is a conversational AI model developed by OpenAI. It’s based on the Transformer architecture and is trained on a large corpus of text data to generate human-like responses to text-based inputs. To understand ChatGPT, you should know the following basics:\u003C\u002Fp\u003E\n\u003Col\u003E\n\u003Cli\u003ENatural Language Processing (NLP): ChatGPT uses NLP techniques to understand and generate human language.\u003C\u002Fli\u003E\n\u003Cli\u003EMachine learning: ChatGPT is a machine learning model that uses supervised learning to generate text responses.\u003C\u002Fli\u003E\n\u003Cli\u003ETransformer architecture: This is a neural network architecture used in NLP tasks that was introduced in the 2017 paper “Attention is All You Need”.\u003C\u002Fli\u003E\n\u003Cli\u003EPre-training: ChatGPT is a pre-trained model that was fine-tuned on a large corpus of text data to generate text responses.\u003C\u002Fli\u003E\n\u003Cli\u003EFine-tuning: This is the process of training a pre-trained model on a smaller, specific task-specific dataset to adapt it to a particular use case.\u003C\u002Fli\u003E\n\u003Cli\u003EGenerative model: ChatGPT is a generative model, which means it generates text rather than simply classifying or translating it.\u003C\u002Fli\u003E\n\u003Cli\u003ELanguage generation: ChatGPT uses probabilistic language generation techniques to generate text responses that are contextually relevant to the input prompt.\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Ch3 id=\"attention-is-all-you-need\"\u003EAttention is All you Need\u003Ca href=\"#attention-is-all-you-need\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003EThe authors argue that attention mechanisms are sufficient for building deep learning models for NLP tasks, such as machine translation, and that these models can be trained effectively in an end-to-end manner.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ESelf-attention mechanisms\u003C\u002Fstrong\u003E allow the model to selectively focus on different parts of the input sequence. This allows the model to process input sequences in parallel, which makes it faster and more parallelizable than RNNs.\u003C\u002Fp\u003E\n\u003Cp\u003EThe Transformer architecture also introduced the concept of \u003Cstrong\u003Emulti-head attention\u003C\u002Fstrong\u003E, which allows the model to attend to different parts of the input sequence in multiple ways, and \u003Cstrong\u003Ethe position encoding mechanism\u003C\u002Fstrong\u003E, which encodes the relative position of words in a sentence.\u003C\u002Fp\u003E\n\u003Ch3 id=\"gpt\"\u003EGPT\u003Ca href=\"#gpt\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003EGPT stands for \u003Cstrong\u003EGenerative Pretrained Transformer\u003C\u002Fstrong\u003E, which is a language generation model developed by OpenAI.\u003C\u002Fp\u003E\n\u003Cp\u003EGPT uses a large corpus of text data to pre-train a deep neural network on the task of \u003Cstrong\u003Epredicting the next word in a sentence\u003C\u002Fstrong\u003E. The pre-training allows the model \u003Cstrong\u003Eto learn general language patterns and relationships\u003C\u002Fstrong\u003E, so that it can generate text that is similar to the training data.\u003C\u002Fp\u003E\n\u003Ch3 id=\"gpt-vs-transformer\"\u003EGPT vs. Transformer\u003Ca href=\"#gpt-vs-transformer\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cul\u003E\n\u003Cli\u003EThe Transformer architecture is a type of neural network that can be used for various NLP tasks.\u003C\u002Fli\u003E\n\u003Cli\u003EGPT is a specific pre-trained language generation model based on the Transformer architecture.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cdiv class=\"code-block python\"\u003E\u003Cpre class='language-python'\u003E\u003Ccode\u003E\u003Cspan class=\"token comment\"\u003E# Simple implementation of Transformer\u003C\u002Fspan\u003E\n\u003Cspan class=\"token keyword\"\u003Eimport\u003C\u002Fspan\u003E torch\n\u003Cspan class=\"token keyword\"\u003Eimport\u003C\u002Fspan\u003E torch\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Enn \u003Cspan class=\"token keyword\"\u003Eas\u003C\u002Fspan\u003E nn\n\u003Cspan class=\"token keyword\"\u003Eimport\u003C\u002Fspan\u003E torch\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Enn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Efunctional \u003Cspan class=\"token keyword\"\u003Eas\u003C\u002Fspan\u003E F\n\n\u003Cspan class=\"token keyword\"\u003Eclass\u003C\u002Fspan\u003E \u003Cspan class=\"token class-name\"\u003ETransformer\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Enn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003EModule\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n    \u003Cspan class=\"token keyword\"\u003Edef\u003C\u002Fspan\u003E \u003Cspan class=\"token function\"\u003E__init__\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eself\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E d_model\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E nhead\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E num_encoder_layers\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E num_decoder_layers\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E dim_feedforward\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E dropout\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token number\"\u003E0.1\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n        \u003Cspan class=\"token builtin\"\u003Esuper\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003ETransformer\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E self\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E__init__\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n\n        self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eencoder_layer \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E nn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003ETransformerEncoderLayer\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Ed_model\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E nhead\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E dim_feedforward\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E dropout\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n        self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Etransformer_encoder \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E nn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003ETransformerEncoder\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eself\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eencoder_layer\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E num_encoder_layers\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n\n        self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Edecoder_layer \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E nn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003ETransformerDecoderLayer\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Ed_model\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E nhead\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E dim_feedforward\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E dropout\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n        self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Etransformer_decoder \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E nn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003ETransformerDecoder\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eself\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Edecoder_layer\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E num_decoder_layers\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n\n    \u003Cspan class=\"token keyword\"\u003Edef\u003C\u002Fspan\u003E \u003Cspan class=\"token function\"\u003Eforward\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eself\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E src\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E tgt\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E src_mask\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token boolean\"\u003ENone\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E tgt_mask\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token boolean\"\u003ENone\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E memory_mask\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token boolean\"\u003ENone\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E src_key_padding_mask\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token boolean\"\u003ENone\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E tgt_key_padding_mask\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token boolean\"\u003ENone\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E memory_key_padding_mask\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token boolean\"\u003ENone\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n        memory \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Etransformer_encoder\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Esrc\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E src_mask\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n        output \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Etransformer_decoder\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Etgt\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E memory\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E tgt_mask\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E memory_mask\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n        \u003Cspan class=\"token keyword\"\u003Ereturn\u003C\u002Fspan\u003E output\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"code-block python\"\u003E\u003Cpre class='language-python'\u003E\u003Ccode\u003E\u003Cspan class=\"token comment\"\u003E# Simple implementation of GPT-1\u003C\u002Fspan\u003E\n\u003Cspan class=\"token keyword\"\u003Eimport\u003C\u002Fspan\u003E torch\n\u003Cspan class=\"token keyword\"\u003Eimport\u003C\u002Fspan\u003E torch\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Enn \u003Cspan class=\"token keyword\"\u003Eas\u003C\u002Fspan\u003E nn\n\u003Cspan class=\"token keyword\"\u003Eimport\u003C\u002Fspan\u003E torch\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Enn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Efunctional \u003Cspan class=\"token keyword\"\u003Eas\u003C\u002Fspan\u003E F\n\n\u003Cspan class=\"token keyword\"\u003Eclass\u003C\u002Fspan\u003E \u003Cspan class=\"token class-name\"\u003EGPT1\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Enn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003EModule\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n    \u003Cspan class=\"token keyword\"\u003Edef\u003C\u002Fspan\u003E \u003Cspan class=\"token function\"\u003E__init__\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eself\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E vocab_size\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E d_model\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E nhead\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E num_layers\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E dim_feedforward\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E dropout\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token number\"\u003E0.1\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n        \u003Cspan class=\"token builtin\"\u003Esuper\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003EGPT1\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E self\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E__init__\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n\n        self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eembedding \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E nn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003EEmbedding\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Evocab_size\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E d_model\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n        self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Etransformer \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E nn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003ETransformer\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Ed_model\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E nhead\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E num_layers\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E dim_feedforward\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E dropout\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n        self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Efc \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E nn\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003ELinear\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Ed_model\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E vocab_size\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n\n    \u003Cspan class=\"token keyword\"\u003Edef\u003C\u002Fspan\u003E \u003Cspan class=\"token function\"\u003Eforward\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eself\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E x\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n        x \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eembedding\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Ex\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n        x \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Etransformer\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Ex\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n        x \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E self\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Efc\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Ex\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n        \u003Cspan class=\"token keyword\"\u003Ereturn\u003C\u002Fspan\u003E x\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Ch3 id=\"text-and-code-embeddings-by-contrastive-pre-training\"\u003EText and Code Embeddings by Contrastive Pre-Training\u003Ca href=\"#text-and-code-embeddings-by-contrastive-pre-training\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003EPaper: \u003C\u002Fp\u003E\n\u003Cp\u003EThe paper proposes a novel pre-training method for learning text and code embeddings. The main idea behind the method is to use a contrastive loss function to pre-train the embeddings in an unsupervised manner.\u003C\u002Fp\u003E\n\u003Cp\u003EThe loss function used for this prediction task is a contrastive loss function that encourages the embeddings of similar tokens (e.g., tokens with similar meanings) to be close together in the embedding space, while encouraging the embeddings of dissimilar tokens to be far apart.\u003C\u002Fp\u003E\n\u003Cp\u003EZero-shot learning is a machine learning approach where a model is trained to recognize and classify new, unseen classes without any additional training data. \u003C\u002Fp\u003E\n\u003Ch3 id=\"autoregressive-model\"\u003EAutoregressive model\u003Ca href=\"#autoregressive-model\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003EAn autoregressive model is a type of statistical model that is used for time series forecasting and sequence generation tasks. In an autoregressive model, the current output or prediction is a function of previous inputs or outputs in the sequence.\u003C\u002Fp\u003E\n\u003Cp\u003EThe most common form of an autoregressive model is the Autoregressive Integrated Moving Average (ARIMA) model, which is used in time series analysis to model the dependencies between observations over time. ARIMA models can be used to perform various tasks, such as prediction of future values, identifying trends and patterns in time series data, and modeling the impact of past events on future outcomes.\u003C\u002Fp\u003E\n\u003Cp\u003EIn the field of machine learning and artificial neural networks, autoregressive models are used for sequence generation tasks, such as text generation and music composition.\u003C\u002Fp\u003E\n\u003Ch3 id=\"zero-shot-learning\"\u003EZero-Shot learning\u003Ca href=\"#zero-shot-learning\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003EThe goal of zero-shot learning is to learn a model that can generalize to new classes based on prior knowledge or information about the relationships between classes, without the need for additional labeled examples of those classes.\u003C\u002Fp\u003E\n\u003Cp\u003EIn zero-shot learning, a model is typically trained on a set of source classes and \u003Cstrong\u003Ea semantic representation\u003C\u002Fstrong\u003E of each class is obtained.\u003C\u002Fp\u003E\n\u003Cp\u003EZero-shot learning is a challenging problem in machine learning, and it is particularly useful for applications where it is not feasible to collect labeled data for every class. For example, in computer vision, zero-shot learning can be used to recognize and classify new species of animals or plants without the need for labeled images of those species.\u003C\u002Fp\u003E\n\u003Cp\u003Ee.g. \u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmtext\u003Ezebras\u003C\u002Fmtext\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmtext\u003Estriped\u003C\u002Fmtext\u003E\u003Cmo\u003E+\u003C\u002Fmo\u003E\u003Cmtext\u003Ehorses\u003C\u002Fmtext\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003E\\text{zebras} = \\text{striped} + \\text{horses}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6944em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord text\"\u003E\u003Cspan class=\"mord\"\u003Ezebras\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord text\"\u003E\u003Cspan class=\"mord\"\u003Estriped\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mbin\"\u003E+\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6944em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord text\"\u003E\u003Cspan class=\"mord\"\u003Ehorses\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Chr\u003E\n\u003Cp\u003ENot from ChatGPT\u003C\u002Fp\u003E\n\u003Ch3 id=\"min-gpt\"\u003Emin GPT\u003Ca href=\"#min-gpt\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fkarpathy\u002FminGPT\u002Fblob\u002Fmaster\u002Fmingpt\u002Fmodel.py\"\u003Ehttps:\u002F\u002Fgithub.com\u002Fkarpathy\u002FminGPT\u002Fblob\u002Fmaster\u002Fmingpt\u002Fmodel.py\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch3 id=\"pros-and-cons\"\u003EPros and Cons\u003Ca href=\"#pros-and-cons\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cul\u003E\n\u003Cli\u003E\u003Cp\u003Etoo generalized &lt;–&gt; more proper answers for everyone\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n"],"uses":{"params":["path","slug"]}}]}
