{"type":"data","nodes":[null,{"type":"data","data":[{"prev":1,"next":4,"section":7},{"slug":2,"title":3},"ml\u002Fchat_gpt","Learn ChatGPT from ChatGPT",{"slug":5,"title":6},"ml\u002Foptimizer","PyTorch - Optimizer",{"file":8,"title":9,"date":10,"summary":-1,"tag":11,"toc":14,"body":15},"ml\u002F22-pytorch.md","PyTorch","Jan. 31, 2023",[12,13],"framework","ML","\u003Ch4\u003EContents\u003C\u002Fh4\u003E\u003Cp class=\"toc-h2\"\u003E\u003Ca href=\"#2x\"\u003E2.x\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#reading-and-updating-attributes\"\u003EReading and updating Attributes\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#hooks-jan-2023\"\u003EHooks (Jan 2023)\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#inference-and-export\"\u003EInference and Export\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#debugging-issues\"\u003EDebugging Issues\u003C\u002Fa\u003E\u003C\u002Fp\u003E","\u003Cp\u003E| update: 2023-01-31, update pytorch 2.0\u003C\u002Fp\u003E\n\u003Ch2 id=\"2x\"\u003E2.x\u003Ca href=\"#2x\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh2\u003E\u003Cp\u003EPyTorch 2.0 offers the same eager-mode development and user experience, while fundamentally changing and supercharging how PyTorch operates \u003Cstrong\u003Eat compiler level under the hood\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EUnderpinning torch.compile are new technologies ‚Äì TorchDynamo, AOTAutograd, PrimTorch and TorchInductor.\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block \"\u003E\u003Cpre class='language-'\u003E\u003Ccode\u003Eüí° We don‚Äôt modify these open-source models except to add a torch.compile call wrapping them.\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fpytorch.org\u002Fassets\u002Fimages\u002Fpytorch-2.0-img4.jpg\" alt=\"https:\u002F\u002Fpytorch.org\u002Fassets\u002Fimages\u002Fpytorch-2.0-img4.jpg\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ETorchDynamo\u003C\u002Fstrong\u003E: Acquiring Graphs reliably and fast\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003Ecaptures PyTorch programs safely using Python Frame Evaluation Hooks.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fpytorch.org\u002Fdocs\u002Fmaster\u002F_images\u002FTorchDynamo.png\" alt=\"https:\u002F\u002Fpytorch.org\u002Fdocs\u002Fmaster\u002F_images\u002FTorchDynamo.png\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EAn approach that uses a CPython feature introduced in PEP-0523 called the Frame Evaluation API. We took a data-driven approach to validate its effectiveness on Graph Capture.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EAOTAutograd\u003C\u002Fstrong\u003E: reusing Autograd for ahead-of-time graphs\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003Eengine as a tracing autodiff for generating ahead-of-time backward traces.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EAOTAutograd leverages PyTorch‚Äôs \u003Ccode\u003Etorch_dispatch\u003C\u002Fcode\u003E extensibility mechanism to trace through our Autograd engine, allowing us to capture the backwards pass ‚Äúahead-of-time‚Äù. This allows us to accelerate both our forwards and backwards pass using TorchInductor.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EPrimTorch\u003C\u002Fstrong\u003E: fast codegen using a define-by-run IR\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003Ea set of ~250 essential operators. (2000+ op. -&gt; ~250 op.)\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EPrim ops with about ~250 operators, which are fairly low-level. These are suited for compilers because they are low-level enough that you need to fuse them back together to get good performance.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ETorchInductor\u003C\u002Fstrong\u003E: ast codegen using a define-by-run IR\u003C\u002Fp\u003E\n\u003Cp\u003ETorchInductor uses a pythonic define-by-run loop level IR to automatically map PyTorch models into generated Triton code on GPUs and C++\u002FOpenMP on CPUs. TorchInductor‚Äôs core loop level IR contains only ~50 operators, and it is implemented in Python, making it easily hackable and extensible.\u003C\u002Fp\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003ETriton, developed by OpenAI\u003Cbr\u003ETriton is a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cdiv class=\"code-block python\"\u003E\u003Cpre class='language-python'\u003E\u003Ccode\u003E\u003Cspan class=\"token comment\"\u003E# API NOT FINAL\u003C\u002Fspan\u003E\n\u003Cspan class=\"token comment\"\u003E# default: optimizes for large models, low compile-time\u003C\u002Fspan\u003E\n\u003Cspan class=\"token comment\"\u003E#          and no extra memory usage\u003C\u002Fspan\u003E\ntorch\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin\"\u003Ecompile\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Emodel\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n\n\u003Cspan class=\"token comment\"\u003E# reduce-overhead: optimizes to reduce the framework overhead\u003C\u002Fspan\u003E\n\u003Cspan class=\"token comment\"\u003E#                and uses some extra memory. Helps speed up small models\u003C\u002Fspan\u003E\ntorch\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin\"\u003Ecompile\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Emodel\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E mode\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token string\"\u003E\"reduce-overhead\"\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n\n\u003Cspan class=\"token comment\"\u003E# max-autotune: optimizes to produce the fastest model,\u003C\u002Fspan\u003E\n\u003Cspan class=\"token comment\"\u003E#               but takes a very long time to compile\u003C\u002Fspan\u003E\ntorch\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin\"\u003Ecompile\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Emodel\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E mode\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token string\"\u003E\"max-autotune\"\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Ch3 id=\"reading-and-updating-attributes\"\u003EReading and updating Attributes\u003Ca href=\"#reading-and-updating-attributes\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003EIf attributes change in certain ways, then TorchDynamo knows to recompile automatically as needed.\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block python\"\u003E\u003Cpre class='language-python'\u003E\u003Ccode\u003E\u003Cspan class=\"token comment\"\u003E# optimized_model works similar to model, feel free to access its attributes and modify them\u003C\u002Fspan\u003E\noptimized_model\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Econv1\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eweight\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Efill_\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"token number\"\u003E0.01\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n\n\u003Cspan class=\"token comment\"\u003E# this change is reflected in model\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Ch3 id=\"hooks-jan-2023\"\u003EHooks (Jan 2023)\u003Ca href=\"#hooks-jan-2023\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003EModule and Tensor hooks don‚Äôt fully work at the moment, but they will eventually work as we finish development.\u003C\u002Fp\u003E\n\u003Ch3 id=\"inference-and-export\"\u003EInference and Export\u003Ca href=\"#inference-and-export\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003EFor model inference, after generating a compiled model using torch.compile, run some warm-up steps before actual model serving. This helps mitigate latency spikes during initial serving.\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block python\"\u003E\u003Cpre class='language-python'\u003E\u003Ccode\u003E\u003Cspan class=\"token comment\"\u003E# API Not Final\u003C\u002Fspan\u003E\nexported_model \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E torch\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E_dynamo\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eexport\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Emodel\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"token builtin\"\u003Einput\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\ntorch\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Esave\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eexported_model\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"token string\"\u003E\"foo.pt\"\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Ch3 id=\"debugging-issues\"\u003EDebugging Issues\u003Ca href=\"#debugging-issues\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fpytorch.org\u002Fdocs\u002Fmaster\u002Fdynamo\u002Ftroubleshooting.html\"\u003Ehttps:\u002F\u002Fpytorch.org\u002Fdocs\u002Fmaster\u002Fdynamo\u002Ftroubleshooting.html\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n"],"uses":{"params":["path","slug"]}}]}
