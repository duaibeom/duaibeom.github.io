{"type":"data","nodes":[null,{"type":"data","data":[{"prev":1,"next":4,"section":7},{"slug":2,"title":3},"blog\u002Fai_safety","AI Safety",{"slug":5,"title":6},"blog\u002Fkaggle_otto","RecSys Competition - OTTO",{"file":8,"title":9,"date":10,"summary":11,"tag":12,"toc":16,"body":17},"blog\u002F16-instruct_gpt.md","InstructGPT","Feb. 24, 2023","Summary of InstructGPT",[13,14,15],"gpt","openai","summary","\u003Ch4\u003EContents\u003C\u002Fh4\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#instructgpt\"\u003EInstructGPT\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#results\"\u003EResults\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#methods\"\u003EMethods\u003C\u002Fa\u003E\u003C\u002Fp\u003E","\u003Cblockquote\u003E\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fopenai.com\u002Fblog\u002Finstruction-following\u002F\"\u003EAligning Language Models to Follow Instructions\u003C\u002Fa\u003E, \u003Ccode\u003EJanuary 27, 2022\u003C\u002Fcode\u003E을 읽고 정리한 글입니다.\u003Cbr\u003E주관적인 번역이 포함되었습니다.\u003Cbr\u003E회색 글자는 개인적인 의견 혹은 정리입니다.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Ch3 id=\"instructgpt\"\u003EInstructGPT\u003Ca href=\"#instructgpt\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003EGPT-3 자체는 사용자가 특정 명령을 입력하면, 올바른 언어와는 별게로 단순히 학습에 의한 다음 단어만 예측하게 된다.\u003C\u002Fp\u003E\n\u003Cp\u003EOpenAI는 언어 모델을 안전하고, 유용하고, 목적성에 맞게 만들기 위해 인간 피드백을 활용한 강화학습(Reinforcement Learning from Human Feedback ,RLHF)을 사용한다. RLHF를 활용하여 목적에 맞게 미세조정한 모델이 InstructGPT이다.\u003C\u002Fp\u003E\n\u003Cp\u003ERLHF 과정은 사용자가 특정 명령을 입력하면 모델이 주는 결과들을 인간이 분석하여 적절한 결과를 선택하거나 우선순위를 정한다. 그러면 모델은 인간이 분석한 결과를 가지고 언어 모델을 미세 조정(fine-tune)한다. 그 결과 InstructGPT는 GPT-3 모델 보다 명령을 주었을 때 위험성 있는 단어를 덜 생성하게 되었다.\u003C\u002Fp\u003E\n\u003Cp\u003EOpenAI는 단순히 동일 모델에서 비교하는 것 외에 더 작은 모델을 fine-tune하여 기존의 큰 모델과 비교하였다. 그 결과 fine-tune한 작은 모델인 \u003Ccode\u003E1.3B InstructGPT\u003C\u002Fcode\u003E 모델이 \u003Ccode\u003E175B GPT-3\u003C\u002Fcode\u003E 모델보다 만족스러웠다.\u003C\u002Fp\u003E\n\u003Caside\u003E\n\n\u003Cp\u003E💡 These InstructGPT models, which have been \u003Cstrong\u003Ein beta on the API for more than a year\u003C\u002Fstrong\u003E, are now the default language models accessible on our API. We believe that fine-tuning language models with \u003Cstrong\u003Ehumans in the loop is a powerful tool for improving their safety and reliability\u003C\u002Fstrong\u003E, and we will continue to push in this direction.\u003C\u002Fp\u003E\n\u003C\u002Faside\u003E\n\n\u003Ch3 id=\"results\"\u003EResults\u003Ca href=\"#results\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003E아래는 여러 데이터를 통해 검증한 결과이다.\u003C\u002Fp\u003E\n\u003Cimg src=\"\u002Fimages\u002Fblog_16_fig_1.png\" height=280\u003E\n\n\u003Cp\u003E언어 유해성은 기존 모델과 비교하여 개선 되었고, 좀 더 정확한 답변을 하게 되었다. 하지만, InstructGPT의 대답 중 모순이지만 진실된 듯 대답하는 (Hallucinations) 현상은 완전히 개선되지 못하였다.\u003C\u002Fp\u003E\n\u003Cp\u003E위의 유해성을 판별하는 요인들은 OpenAI의 \u003Ca href=\"https:\u002F\u002Fplatform.openai.com\u002Fdocs\u002Fapi-reference\u002Fmoderations\"\u003EModeration API\u003C\u002Fa\u003E를 통해 간략히 알 수 있다.\u003C\u002Fp\u003E\n\u003Cp\u003EModeration API에 요청:\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block bash\"\u003E\u003Cpre class='language-bash'\u003E\u003Ccode\u003E\u003Cspan class=\"token function\"\u003Ecurl\u003C\u002Fspan\u003E https:\u002F\u002Fapi.openai.com\u002Fv1\u002Fmoderations \u003Cspan class=\"token punctuation\"\u003E\\\u003C\u002Fspan\u003E\n  \u003Cspan class=\"token parameter variable\"\u003E-H\u003C\u002Fspan\u003E \u003Cspan class=\"token string\"\u003E'Content-Type: application\u002Fjson'\u003C\u002Fspan\u003E \u003Cspan class=\"token punctuation\"\u003E\\\u003C\u002Fspan\u003E\n  \u003Cspan class=\"token parameter variable\"\u003E-H\u003C\u002Fspan\u003E \u003Cspan class=\"token string\"\u003E'Authorization: Bearer YOUR_API_KEY'\u003C\u002Fspan\u003E \u003Cspan class=\"token punctuation\"\u003E\\\u003C\u002Fspan\u003E\n  \u003Cspan class=\"token parameter variable\"\u003E-d\u003C\u002Fspan\u003E \u003Cspan class=\"token string\"\u003E'{\n  \"input\": \"I want to kill them.\"\n}'\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003EModeration API의 결과:\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block bash\"\u003E\u003Cpre class='language-bash'\u003E\u003Ccode\u003E\u003Cspan class=\"token punctuation\"\u003E{\u003C\u002Fspan\u003E\n  \u003Cspan class=\"token string\"\u003E\"id\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token string\"\u003E\"modr-5MWoLO\"\u003C\u002Fspan\u003E,\n  \u003Cspan class=\"token string\"\u003E\"model\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token string\"\u003E\"text-moderation-001\"\u003C\u002Fspan\u003E,\n  \u003Cspan class=\"token string\"\u003E\"results\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token punctuation\"\u003E[\u003C\u002Fspan\u003E\n    \u003Cspan class=\"token punctuation\"\u003E{\u003C\u002Fspan\u003E\n      \u003Cspan class=\"token string\"\u003E\"categories\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token punctuation\"\u003E{\u003C\u002Fspan\u003E\n        \u003Cspan class=\"token string\"\u003E\"hate\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E false,\n        \u003Cspan class=\"token string\"\u003E\"hate\u002Fthreatening\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E true,\n        \u003Cspan class=\"token string\"\u003E\"self-harm\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E false,\n        \u003Cspan class=\"token string\"\u003E\"sexual\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E false,\n        \u003Cspan class=\"token string\"\u003E\"sexual\u002Fminors\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E false,\n        \u003Cspan class=\"token string\"\u003E\"violence\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E true,\n        \u003Cspan class=\"token string\"\u003E\"violence\u002Fgraphic\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token boolean\"\u003Efalse\u003C\u002Fspan\u003E\n      \u003Cspan class=\"token punctuation\"\u003E}\u003C\u002Fspan\u003E,\n      \u003Cspan class=\"token string\"\u003E\"category_scores\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token punctuation\"\u003E{\u003C\u002Fspan\u003E\n        \u003Cspan class=\"token string\"\u003E\"hate\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token number\"\u003E0.22714105248451233\u003C\u002Fspan\u003E,\n        \u003Cspan class=\"token string\"\u003E\"hate\u002Fthreatening\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token number\"\u003E0.4132447838783264\u003C\u002Fspan\u003E,\n        \u003Cspan class=\"token string\"\u003E\"self-harm\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token number\"\u003E0.005232391878962517\u003C\u002Fspan\u003E,\n        \u003Cspan class=\"token string\"\u003E\"sexual\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token number\"\u003E0.01407341007143259\u003C\u002Fspan\u003E,\n        \u003Cspan class=\"token string\"\u003E\"sexual\u002Fminors\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token number\"\u003E0.0038522258400917053\u003C\u002Fspan\u003E,\n        \u003Cspan class=\"token string\"\u003E\"violence\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token number\"\u003E0.9223177433013916\u003C\u002Fspan\u003E,\n        \u003Cspan class=\"token string\"\u003E\"violence\u002Fgraphic\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token number\"\u003E0.036865197122097015\u003C\u002Fspan\u003E\n      \u003Cspan class=\"token punctuation\"\u003E}\u003C\u002Fspan\u003E,\n      \u003Cspan class=\"token string\"\u003E\"flagged\"\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin class-name\"\u003E:\u003C\u002Fspan\u003E \u003Cspan class=\"token boolean\"\u003Etrue\u003C\u002Fspan\u003E\n    \u003Cspan class=\"token punctuation\"\u003E}\u003C\u002Fspan\u003E\n  \u003Cspan class=\"token punctuation\"\u003E]\u003C\u002Fspan\u003E\n\u003Cspan class=\"token punctuation\"\u003E}\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Ch3 id=\"methods\"\u003EMethods\u003Ca href=\"#methods\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cblockquote\u003E\n\u003Cp\u003ELabeler는 모델을 목적에 맞게 훈련될 수 있도록 데이터 생성 및 조정, 모델 결과를 검토하는 전문가이다. Labeler에 포함되는 사람들은 목적에 맞는 배경지식을 가진 사람 혹은 깊은 관심을 가지는 예비 사용자가 될 수 있다.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cimg src=\"\u002Fimages\u002Fblog_16_fig_2.png\"\u003E\n\n\u003Cp\u003E전체적인 과정은 아래와 같다.\u003C\u002Fp\u003E\n\u003Col\u003E\n\u003Cli\u003E언어 모델 지도학습 (Supervised Fine-Tune)\u003Col\u003E\n\u003Cli\u003E기존의 입력된 명령들 중 적절한 경우들을 선택하여 테스트 환경을 정의한다.\u003C\u002Fli\u003E\n\u003Cli\u003ELabler는 사전 정의한 명령 기반으로 모델의 output과 자신의 지식을 활용하여 이상적인 결과를 데이터로 생성한다.\u003C\u002Fli\u003E\n\u003Cli\u003E생성된 데이터를 활용하여 GPT-3 모델을 fine-tune 한다.\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EReward 모델 구성\u003Col\u003E\n\u003Cli\u003E명령어와 해당 명령어의 결과들이 하나의 세트로 구성된 데이터를 준비한다.\u003C\u002Fli\u003E\n\u003Cli\u003ELabeler가 생성된 결과들을 비교하여 순위를 정한다.\u003C\u002Fli\u003E\n\u003Cli\u003E해당 순위를 바탕으로 reward 모델을 학습한다.\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E강화학습\u003Col\u003E\n\u003Cli\u003E테스트 환경에 사용된 명령들이 아닌 새로운 명령들로 테스트한다.\u003C\u002Fli\u003E\n\u003Cli\u003EPPO 모델을 활용하여, 언어 모델의 행동들을 정의한다.\u003C\u002Fli\u003E\n\u003Cli\u003EReward 모델을 사용하여 해당 결과를 평가한다.\u003C\u002Fli\u003E\n\u003Cli\u003E평가 결과를 바탕으로 PPO 모델에 반영하여, 언어 모델의 행동을 조정한다.\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Caside\u003E\n\n\u003Cp\u003E💡 \u003Cstrong\u003ERLHF uses human preferences\u003C\u002Fstrong\u003E as a reward signal to fine-tune our models, which is important as the safety and alignment problems we are aiming to solve are complex and subjective, and \u003Cstrong\u003Earen’t fully captured by simple automatic metrics\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\n\u003C\u002Faside\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003E강화학습을 사용한 이유는 간단한 평가 방법 하나로 인간의 복잡한 선호를 학습할 수 없기 때문이다. 복잡한 정의를 모두 할 수 없기 때문에 강화학습을 통해 이것을 정의하고자 하였다. 비용과 편의성을 모두 고려한 선택이다.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003E하지만, 위의 과정 중 지도학습 과정에 크게 비중을 두지 않았다. 이유는 GPT-3의 가능성을 제한할까 걱정이 되었다. 지도학습으로 인해 다른 영역의 기능이 떨어지는 기회비용(Alignment tax)이 발생할 수 있었다. 그래서 기존 학습(pre-train)에 비하여 2% 데이터와 학습 시간을 사용하였다. 이와 더불어, 강화 학습을 이용해 fine-tune 하는 과정에서 정의한 테스트 환경 데이터 외에 기존에 사용된 학습 데이터를 일정 비율 섞어 같이 학습함으로 다른 영역의 기능을 유지하면서 목적에 맞게 학습할 수 있게 되었다.\u003C\u002Fp\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003E기존 데이터를 섞어 학습할 때 기존 데이터는 the normal log likelihood maximization를 사용하여 최적화 하였다. 강화 학습에서는 Reward term을 maximization 하는 것이 목적이기 때문에 이와 맞추기 위해 maximization 방식을 그대로 사용한 것으로 보인다. 이와 다른 방법으로 KL Divergence를 사용하여 Reward 방식과 기존 학습 방식을 비슷하게 유지할 수 있지만, 실험 결과 사전 학습 데이터 자체를 likelihood maximization하는 것이 더 효과적이었다.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003E위의 학습을 통한 모델은 모든 상황에 대하여 보증하기 어렵다. 그래도, InstructGPT 모델 과 Reward 모델 모두 특정 상황에 편향되지 않았다. 특정 영역에서 훈련된 모델을 다른 영역에 테스트 하였을 때 일반화된 성능을 보여주었다. 이것은 단편적일 수 있지만, 더 넓은 영역에서 어떤 영향을 줄지는 연구가 필요하다.\u003C\u002Fp\u003E\n\u003Cp\u003E현재 모델이 문화적으로 Labeler와 English 데이터에 편향되어 있을 수 있다. 그리고 사용자 그룹에 따라 결과에 영향을 받는 정도가 다르기 때문에 이 부분 역시 완화해야할 부분이다.\u003C\u002Fp\u003E\n"],"uses":{"params":["path","slug"]}}]}
