{"type":"data","nodes":[null,{"type":"data","data":[{"prev":1,"next":4,"section":7},{"slug":2,"title":3},"cs\u002Flinux","Linux",{"slug":5,"title":6},"cs\u002Fpython3","Python3",{"file":8,"title":9,"date":10,"summary":-1,"tag":11,"toc":14,"body":15},"cs\u002F02-glossary.md","Glossary","Feb. 26, 2023",[12,13],"cs","","\u003Ch4\u003EContents\u003C\u002Fh4\u003E\u003Cp class=\"toc-h3\"\u003E\u003Ca href=\"#floating-point-format\"\u003EFloating-point format\u003C\u002Fa\u003E\u003C\u002Fp\u003E","\u003Ch3 id=\"floating-point-format\"\u003EFloating-point format\u003Ca href=\"#floating-point-format\" class=\"anchor\"\u003E\u003Cspan class=\"visually-hidden\"\u003E#\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fh3\u003E\u003Cp\u003E\u003Ccode\u003Efloat16\u003C\u002Fcode\u003E (Half-precision floating-point): (1 sign, 5 exponent, 10 mantissa) bits\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode\u003Ebfloat16\u003C\u002Fcode\u003E (Brain floating-point 16): (1 sign, \u003Cem\u003E8 exponent\u003C\u002Fem\u003E, 7 mantissa) bits\u003Cbr\u003E(developed by Google for use in deep learning)\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode\u003Efloat32\u003C\u002Fcode\u003E (Single-precision floating-point): (1 sign, 8 exponent, 23 mantissa) bits\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode\u003Efloat64\u003C\u002Fcode\u003E (Double-precision floating-point): (1 sign, 11 exponent, 52 mantissa) bits\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode\u003Efloat128\u003C\u002Fcode\u003E (Quadruple-precision floating-point): (1 sign, 15 exponent, 112 mantissa) bits\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode\u003Efloat256\u003C\u002Fcode\u003E (Octuple-precision floating-point): (1 sign, 19 exponent, 236 mantissa) bits\u003C\u002Fp\u003E\n"],"uses":{"params":["path","slug"]}}]}
